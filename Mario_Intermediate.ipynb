{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gym\n",
        "!pip install stable-baselines3\n",
        "!pip install gym-super-mario-bros\n",
        "!pip install opencv-python\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZXQVyhpYIBL",
        "outputId": "8a02b25a-970a-4dba-8001-b7856e8aed0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: stable-baselines3 in /usr/local/lib/python3.10/dist-packages (2.2.1)\n",
            "Requirement already satisfied: gymnasium<0.30,>=0.28.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (2.1.0+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (2.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (1.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (3.7.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3) (4.5.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (2.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->stable-baselines3) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable-baselines3) (1.3.0)\n",
            "Collecting gym-super-mario-bros\n",
            "  Downloading gym_super_mario_bros-7.4.0-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nes-py>=8.1.4 (from gym-super-mario-bros)\n",
            "  Downloading nes_py-8.2.1.tar.gz (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.7/77.7 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: gym>=0.17.2 in /usr/local/lib/python3.10/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros) (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros) (1.23.5)\n",
            "Collecting pyglet<=1.5.21,>=1.4.0 (from nes-py>=8.1.4->gym-super-mario-bros)\n",
            "  Downloading pyglet-1.5.21-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.10/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros) (4.66.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros) (0.0.8)\n",
            "Building wheels for collected packages: nes-py\n",
            "  Building wheel for nes-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nes-py: filename=nes_py-8.2.1-cp310-cp310-linux_x86_64.whl size=535719 sha256=138c60ea54bdb0471862643c1ad295bd908fa52d5149e3b8839be993e53b0b35\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/a7/d5/9aa14b15df740a53d41f702e4c795731b6c4da7925deb8476c\n",
            "Successfully built nes-py\n",
            "Installing collected packages: pyglet, nes-py, gym-super-mario-bros\n",
            "Successfully installed gym-super-mario-bros-7.4.0 nes-py-8.2.1 pyglet-1.5.21\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.23.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import gym\n",
        "import gym_super_mario_bros\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
        "\n",
        "\n",
        "class smb_grid:\n",
        "\n",
        "    def __init__(self, env):\n",
        "        self.ram = env.unwrapped.ram\n",
        "        self.screen_size_x = 16     # rendered screen size\n",
        "        self.screen_size_y = 13\n",
        "\n",
        "        self.mario_level_x = self.ram[0x6d]*256 + self.ram[0x86]\n",
        "        self.mario_x = self.ram[0x3ad]  # mario's position on the rendered screen\n",
        "        self.mario_y = self.ram[0x3b8] + 16 # top edge of (big) mario\n",
        "\n",
        "        self.x_start = self.mario_level_x - self.mario_x # left edge pixel of the rendered screen in level\n",
        "        self.rendered_screen = self.get_rendered_screen()\n",
        "\n",
        "\n",
        "    ########\n",
        "    # get background tile grid\n",
        "\n",
        "    def tile_loc_to_ram_address(self, x, y):\n",
        "        '''\n",
        "        convert (x, y) in Current tile (32x13, stored as 16x26 in ram) to ram address\n",
        "        x: 0 to 31\n",
        "        y: 0 to 12\n",
        "        '''\n",
        "        page = x // 16\n",
        "        x_loc = x%16\n",
        "        y_loc = page*13 + y\n",
        "\n",
        "        address = 0x500 + x_loc + y_loc*16\n",
        "\n",
        "        return address\n",
        "\n",
        "    def get_rendered_screen(self):\n",
        "        '''\n",
        "        Get the rendered screen (16 x 13) from ram\n",
        "        empty: 0\n",
        "        tile: 1\n",
        "        enemy: -1\n",
        "        mario: 2\n",
        "        '''\n",
        "\n",
        "        # Get background tiles\n",
        "\n",
        "        rendered_screen = np.zeros((self.screen_size_y, self.screen_size_x))\n",
        "        screen_start = int(np.rint(self.x_start / 16))\n",
        "\n",
        "        for i in range(self.screen_size_x):\n",
        "            for j in range(self.screen_size_y):\n",
        "                x_loc = (screen_start + i) % (self.screen_size_x * 2)\n",
        "                y_loc = j\n",
        "                address = self.tile_loc_to_ram_address(x_loc, y_loc)\n",
        "                #bg_screen2[j, i] = env.unwrapped.ram[address]\n",
        "\n",
        "                # Convert all types of tile to 1\n",
        "                if self.ram[address] != 0:\n",
        "                    rendered_screen[j, i] = 1\n",
        "\n",
        "        # Add mario\n",
        "        x_loc = (self.mario_x + 8) // 16\n",
        "        y_loc = (self.mario_y - 32) // 16 # top 2 rows in the rendered screen aren't stored in ram\n",
        "        if x_loc < 16 and y_loc < 13:\n",
        "            rendered_screen[y_loc, x_loc] = 2\n",
        "\n",
        "        # Add enemies\n",
        "        for i in range(5):\n",
        "            # check if the enemy is drawn\n",
        "            if self.ram[0xF + i] == 1:\n",
        "                enemy_x = self.ram[0x6e + i]*256 + self.ram[0x87 + i] - self.x_start\n",
        "                enemy_y = self.ram[0xcf + i]\n",
        "                x_loc = (enemy_x + 8) // 16\n",
        "                y_loc = (enemy_y + 8 - 32) // 16\n",
        "\n",
        "                # check if the enemy is inside the rendered screen\n",
        "                # 8/6/22 fixed bug where enemy with x_loc < 0 still got added to rendered_screen; doesn't seem to affect trained models' performance\n",
        "                # if x_loc < 16 and y_loc < 13:\n",
        "                if 0 <= x_loc < 16 and 0 <= y_loc < 13:\n",
        "                    rendered_screen[y_loc, x_loc] = -1\n",
        "\n",
        "        return rendered_screen"
      ],
      "metadata": {
        "id": "u2n19gSQkjQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "\n",
        "# Define custom environment to crop obs frame\n",
        "from gym import spaces\n",
        "import gym_super_mario_bros\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, RIGHT_ONLY\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.policies import obs_as_tensor\n",
        "\n",
        "\n",
        "\n",
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors\n",
        "from matplotlib import animation\n",
        "\n",
        "import imageio\n",
        "\n",
        "\n",
        "class SMBRamWrapper(gym.ObservationWrapper):\n",
        "    def __init__(self, env, crop_dim=[0, 16, 0, 13], n_stack=4, n_skip=2):\n",
        "        '''\n",
        "        crop_dim: [x0, x1, y0, y1]\n",
        "        obs shape = (height, width, n_stack), n_stack=0 is the most recent frame\n",
        "        n_skip: e.g. n_stack=4, n_skip=2, use frames [0, 2, 4, 6]\n",
        "        '''\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.crop_dim = crop_dim\n",
        "        self.n_stack = n_stack\n",
        "        self.n_skip = n_skip\n",
        "        # Modified from stable_baselines3.common.atari_wrappers.WarpFrame()\n",
        "        # https://stable-baselines3.readthedocs.io/en/master/_modules/stable_baselines3/common/atari_wrappers.html#AtariWrapper\n",
        "        self.width = crop_dim[1] - crop_dim[0]\n",
        "        self.height = crop_dim[3] - crop_dim[2]\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=-1, high=2, shape=(self.height, self.width, self.n_stack), dtype=int\n",
        "        )\n",
        "\n",
        "        self.frame_stack = np.zeros((self.height, self.width, (self.n_stack-1)*self.n_skip+1))\n",
        "        #self.INDEX_SKIP = 1\n",
        "\n",
        "    def observation(self, obs):\n",
        "        grid = smb_grid(self.env)\n",
        "        frame = grid.rendered_screen # 2d array\n",
        "        frame = self.crop_obs(frame)\n",
        "\n",
        "        self.frame_stack[:,:,1:] = self.frame_stack[:,:,:-1] # shift frame_stack by 1\n",
        "        self.frame_stack[:,:,0] = frame # add current frame to stack\n",
        "        obs = self.frame_stack[:,:,::self.n_skip]\n",
        "        return obs\n",
        "\n",
        "    def reset(self):\n",
        "        obs = self.env.reset()\n",
        "        self.frame_stack = np.zeros((self.height, self.width, (self.n_stack-1)*self.n_skip+1))\n",
        "        grid = smb_grid(self.env)\n",
        "        frame = grid.rendered_screen # 2d array\n",
        "        frame = self.crop_obs(frame)\n",
        "        for i in range(self.frame_stack.shape[-1]):\n",
        "            self.frame_stack[:,:,i] = frame\n",
        "        obs = self.frame_stack[:,:,::self.n_skip]\n",
        "        return obs\n",
        "\n",
        "    def crop_obs(self, im):\n",
        "        '''\n",
        "        Crop observed frame image to reduce input size\n",
        "        Returns cropped_frame = original_frame[y0:y1, x0:x1]\n",
        "        '''\n",
        "        [x0, x1, y0, y1] = self.crop_dim\n",
        "        im_crop = im[y0:y1, x0:x1]\n",
        "        return im_crop\n",
        "\n",
        "\n",
        "def load_smb_env(name='SuperMarioBros-1-1-v0', crop_dim=[0,16,0,13], n_stack=2, n_skip=4):\n",
        "    '''\n",
        "    Wrapper function for loading and processing smb env\n",
        "    '''\n",
        "    env = gym_super_mario_bros.make(name)\n",
        "    env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
        "    env_wrap = SMBRamWrapper(env, crop_dim, n_stack=n_stack, n_skip=n_skip)\n",
        "    env_wrap = DummyVecEnv([lambda: env_wrap])\n",
        "\n",
        "    return env_wrap\n",
        "\n",
        "\n",
        "class SMB():\n",
        "    '''\n",
        "    Wrapper function containing the processed environment and the loaded model\n",
        "    '''\n",
        "    def __init__(self, env, model):\n",
        "        self.env = env\n",
        "        self.model = model\n",
        "\n",
        "    def play(self, episodes=5, deterministic=False, render=True, return_eval=False):\n",
        "        for episode in range(1, episodes+1):\n",
        "            states = self.env.reset()\n",
        "            done = False\n",
        "            score = 0\n",
        "\n",
        "            if render == True:\n",
        "                while not done:\n",
        "                    self.env.render()\n",
        "                    action, _ = self.model.predict(states, deterministic=deterministic)\n",
        "                    states, reward, done, info = self.env.step(action)\n",
        "                    score += reward\n",
        "                    time.sleep(0.01)\n",
        "                print('Episode:{} Score:{}'.format(episode, score))\n",
        "            else:\n",
        "                while not done:\n",
        "                    action, _ = self.model.predict(states, deterministic=deterministic)\n",
        "                    states, reward, done, info = self.env.step(action)\n",
        "                    score += reward\n",
        "        if return_eval == True:\n",
        "            return score, info\n",
        "        else:\n",
        "            return\n",
        "\n",
        "    def evaluate(self, episodes=20, deterministic=False):\n",
        "        '''\n",
        "        returns rewards, steps (both have length [episodes])\n",
        "        '''\n",
        "        rewards, steps = evaluate_policy(self.model, self.env, n_eval_episodes=episodes,\n",
        "                                 deterministic=deterministic, render=False,\n",
        "                                 return_episode_rewards=True)\n",
        "        return rewards, steps\n",
        "\n",
        "    import numpy as np\n",
        "\n",
        "\n",
        "    def predict_proba(self, state):\n",
        "        '''\n",
        "        Predict the probability of each action given a state\n",
        "        https://stackoverflow.com/questions/66428307/how-to-get-action-propability-in-stable-baselines-3/70012691#70012691?newreg=bd5479b970664069b359903e0151b4a1\n",
        "        '''\n",
        "        model = self.model\n",
        "        obs = obs_as_tensor(state, model.policy.device)\n",
        "        dis = model.policy.get_distribution(obs)\n",
        "        probs = dis.distribution.probs\n",
        "        probs_np = probs.detach().numpy()\n",
        "        return probs_np\n",
        "\n",
        "    #############\n",
        "    # functions for making plots & videos\n",
        "\n",
        "    def make_video_frames(self, deterministic=False):\n",
        "        '''\n",
        "        For each step, plot obs & rendered screen in one figure for making videoes\n",
        "        '''\n",
        "        state = self.env.reset()\n",
        "        done = False\n",
        "        score = [0]\n",
        "        #self._make_combined_plot2(state, score, prob_actions)\n",
        "        #self._make_combined_plot(state, score)\n",
        "\n",
        "\n",
        "        while not done:\n",
        "        #for i in range(1):\n",
        "            prob_actions = self.predict_proba(state)\n",
        "            action, _ = self.model.predict(state, deterministic=deterministic)\n",
        "            state, reward, done, info = self.env.step(action)\n",
        "            score += reward\n",
        "            self._make_combined_plot2(state, score, prob_actions)\n",
        "            #self._make_combined_plot(state, score)\n",
        "\n",
        "\n",
        "    def _make_combined_plot2(self, state, score, prob_actions):\n",
        "        '''\n",
        "        Originally made for n_stack = 4 & n_skip = 4, SIMPLE_MOVEMENT\n",
        "        '''\n",
        "        # get rendered screen\n",
        "        im_render = self.env.render(mode=\"rgb_array\")\n",
        "\n",
        "        n_stack = state.shape[-1]\n",
        "        cmap = colors.ListedColormap(['red', 'skyblue', 'brown', 'blue'])\n",
        "        bounds = [-1.5, -0.5, 0.5, 1.5, 2.5]\n",
        "        norm = colors.BoundaryNorm(bounds, cmap.N)\n",
        "\n",
        "        #obs_loc = [[0, 1], [0, 2], [1, 1], [1, 2]]\n",
        "        obs_loc = [[0, 1], [1, 1], [2, 1], [3, 1]]\n",
        "        obs_text = ['t (current frame)', 't-4', 't-8', 't-12']\n",
        "        action_list = ['NOOP', 'right', 'right+A', 'right+B', 'right+A+B', 'A', 'left']\n",
        "\n",
        "\n",
        "        ##########\n",
        "        fig = plt.figure(dpi=100, figsize=(6, 6), constrained_layout=False, tight_layout=True)\n",
        "        gs = fig.add_gridspec(4, 2, width_ratios=[3, 1])\n",
        "\n",
        "        # individual obs frames\n",
        "        for n in range(n_stack):\n",
        "            ax = fig.add_subplot(gs[obs_loc[n][0], obs_loc[n][1]])\n",
        "            im = ax.imshow(state[0,:,:,n], cmap=cmap, norm=norm)\n",
        "            ax.set_axis_off()\n",
        "            ax.text(-0.5, 14.5, obs_text[n])\n",
        "\n",
        "        # prob_actions\n",
        "        ax = fig.add_subplot(gs[3, 0])\n",
        "        ax.bar(action_list, prob_actions[0])\n",
        "        plt.xticks(rotation=45)\n",
        "        ax.set_ylim(0, 1.05)\n",
        "\n",
        "        # rendered screen\n",
        "        ax = fig.add_subplot(gs[0:3, 0])\n",
        "        im = ax.imshow(im_render)\n",
        "        ax.set_axis_off()\n",
        "        ax.text(0, -5, 'score: '+str(int(score[0])))\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    def _make_combined_plot(self, state, score):\n",
        "        # get rendered screen\n",
        "        im_render = self.env.render(mode=\"rgb_array\")\n",
        "        n_stack = state.shape[-1]\n",
        "\n",
        "        cmap = colors.ListedColormap(['red', 'skyblue', 'brown', 'blue'])\n",
        "        bounds = [-1.5, -0.5, 0.5, 1.5, 2.5]\n",
        "        norm = colors.BoundaryNorm(bounds, cmap.N)\n",
        "\n",
        "        #obs_text = ['t (current frame)', 't-4', 't-8', 't-12']\n",
        "\n",
        "        fig = plt.figure(dpi=100, figsize=(5.5, 4), constrained_layout=False, tight_layout=True)\n",
        "        gs = fig.add_gridspec(4, 2, width_ratios=[4, 1])\n",
        "\n",
        "        # individual obs frames\n",
        "        for n in range(n_stack):\n",
        "            ax = fig.add_subplot(gs[n, 1])\n",
        "            im = ax.imshow(states[0,:,:,n], cmap=cmap, norm=norm)\n",
        "            ax.set_axis_off()\n",
        "\n",
        "        # rendered screen\n",
        "        ax = fig.add_subplot(gs[:, 0])\n",
        "        im = ax.imshow(im_render)\n",
        "        ax.set_axis_off()\n",
        "        ax.text(0, -5, 'score: '+str(int(score[0])))\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    def make_animation(self, deterministic=True, filename='gym_animation.gif', RETURN_FRAMES=False):\n",
        "        '''\n",
        "        Make an animation of the rendered screen\n",
        "        '''\n",
        "        # run policy\n",
        "        frames = []\n",
        "        states = self.env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            #frames.append(self.env.render(mode=\"rgb_array\"))\n",
        "            im = self.env.render(mode=\"rgb_array\")\n",
        "            frames.append(im.copy())\n",
        "            action, _ = self.model.predict(states, deterministic=deterministic)\n",
        "            states, reward, done, info = self.env.step(action)\n",
        "\n",
        "        if RETURN_FRAMES == False:\n",
        "            # make animation\n",
        "            imageio.mimsave(filename, frames, fps=50)\n",
        "        else: # make animation manually in case Mario gets stuck in the level and drags the animation for too long\n",
        "            return frames"
      ],
      "metadata": {
        "id": "2YF1xkU3kXen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBuARerkX-Hb",
        "outputId": "c0c8edb5-e9bf-43d4-cd70-455ab8d1da3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-1-1-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "from nes_py.wrappers import JoypadSpace\n",
        "import gym\n",
        "import gym_super_mario_bros\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, RIGHT_ONLY\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecEnv, SubprocVecEnv\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')\n",
        "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup cropping size\n",
        "x0 = 0\n",
        "x1 = 16\n",
        "y0 = 0\n",
        "y1 = 13\n",
        "n_stack = 4\n",
        "n_skip = 4\n",
        "\n",
        "env_wrap = SMBRamWrapper(env, [x0, x1, y0, y1], n_stack=n_stack, n_skip=n_skip)\n",
        "\n",
        "# test env_wrap\n",
        "done = True\n",
        "for i in range(150):\n",
        "    if done:\n",
        "        state = env_wrap.reset()\n",
        "    state, reward, done, info = env_wrap.step(env_wrap.action_space.sample())\n",
        "\n",
        "state.shape\n",
        "\n",
        "(13, 16, 4)\n",
        "\n",
        "fig, ax = plt.subplots(1, n_stack, figsize=(14,10))\n",
        "for i in range(n_stack):\n",
        "    ax[i].imshow(state[:,:,n_stack-i-1], vmin=-1, vmax=2)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "IFasNi99k1yq",
        "outputId": "e1561ca6-5fc1-4928-f1d7-0a5c299714e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1400x1000 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply other wrapper functions\n",
        "env_wrap = Monitor(env_wrap)  # for tensorboard log\n",
        "env_wrap = DummyVecEnv([lambda: env_wrap])"
      ],
      "metadata": {
        "id": "USrwOpSEk75y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Callbacks\n",
        "\n",
        "# Save intermediate models\n",
        "# Copied from Nicholas Renotte's code\n",
        "class TrainAndLoggingCallback(BaseCallback):\n",
        "\n",
        "    def __init__(self, check_freq, save_path,\n",
        "                 starting_steps=0, verbose=1):\n",
        "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
        "        self.check_freq = check_freq\n",
        "        self.save_path = save_path\n",
        "        self.starting_steps = starting_steps\n",
        "\n",
        "    def _init_callback(self):\n",
        "        if self.save_path is not None:\n",
        "            os.makedirs(self.save_path, exist_ok=True)\n",
        "\n",
        "    def _on_step(self):\n",
        "        if self.n_calls % self.check_freq == 0:\n",
        "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls + int(self.starting_steps)))\n",
        "            self.model.save(model_path)\n",
        "\n",
        "        return True\n",
        "\n",
        "# Linear learning rate schedule\n",
        "# https://stable-baselines3.readthedocs.io/en/master/guide/examples.html#learning-rate-schedule\n",
        "from typing import Callable\n",
        "\n",
        "def linear_schedule(initial_value: float) -> Callable[[float], float]:\n",
        "    \"\"\"\n",
        "    Linear learning rate schedule.\n",
        "\n",
        "    :param initial_value: Initial learning rate.\n",
        "    :return: schedule that computes\n",
        "      current learning rate depending on remaining progress\n",
        "    \"\"\"\n",
        "    def func(progress_remaining: float) -> float:\n",
        "        \"\"\"\n",
        "        Progress will decrease from 1 (beginning) to 0.\n",
        "\n",
        "        :param progress_remaining:\n",
        "        :return: current learning rate\n",
        "        \"\"\"\n",
        "        return progress_remaining * initial_value\n",
        "\n",
        "    return func"
      ],
      "metadata": {
        "id": "LKhDW8iPk_1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### MODIFY THESE TWO DIRECTORIES BEFORE TRAINING A NEW MODEL ###\n",
        "MODEL_DIR = './models/NEW_MODEL_DIR'\n",
        "LOG_DIR = './logs/NEW_LOG_DIR'"
      ],
      "metadata": {
        "id": "q-GIUyEtlCWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import dependencies\n",
        "import gym\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecEnv\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "\n",
        "import os"
      ],
      "metadata": {
        "id": "-KeiWkOtnAdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "-Ie1KN9doOrf",
        "outputId": "98f44b21-6bbb-4588-da36-6370107fe87b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-81567b26-a958-4134-84c7-1ff5ab1337d8\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-81567b26-a958-4134-84c7-1ff5ab1337d8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving pre-trained-1.zip to pre-trained-1.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select a pre-trained model\n",
        "#MODEL_DIR = './models'\n",
        "\n",
        "# obs = 4 frames\n",
        "crop_dim = [0, 16, 0, 13]\n",
        "n_stack = 4\n",
        "n_skip = 4\n",
        "MODEL_NAME = 'pre-trained-1'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gv7Co6Tnl6f",
        "outputId": "e8edcfdb-5c77-4a4f-9255-77ddf0142751"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load env and model\n",
        "env_wrap = load_smb_env('SuperMarioBros-1-1-v0', crop_dim, n_stack, n_skip)\n",
        "model = PPO(MODEL_NAME, env=env_wrap)\n",
        "smb = SMB(env_wrap, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "S79edfKmnqf5",
        "outputId": "864de210-e356-47c5-f829-9ad9da7cd9ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-c640b23a1766>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load env and model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0menv_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_smb_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SuperMarioBros-1-1-v0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrop_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_stack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_skip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPPO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv_wrap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0msmb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSMB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/ppo/ppo.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, policy, env, learning_rate, n_steps, batch_size, n_epochs, gamma, gae_lambda, clip_range, clip_range_vf, normalize_advantage, ent_coef, vf_coef, max_grad_norm, use_sde, sde_sample_freq, rollout_buffer_class, rollout_buffer_kwargs, target_kl, stats_window_size, tensorboard_log, policy_kwargs, verbose, seed, device, _init_setup_model)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0m_init_setup_model\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     ):\n\u001b[0;32m--> 109\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    110\u001b[0m             \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, policy, env, learning_rate, n_steps, gamma, gae_lambda, ent_coef, vf_coef, max_grad_norm, use_sde, sde_sample_freq, rollout_buffer_class, rollout_buffer_kwargs, stats_window_size, tensorboard_log, monitor_wrapper, policy_kwargs, verbose, seed, device, _init_setup_model, supported_action_spaces)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0msupported_action_spaces\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mType\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mspaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSpace\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     ):\n\u001b[0;32m---> 85\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m     86\u001b[0m             \u001b[0mpolicy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/base_class.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, policy, env, learning_rate, policy_kwargs, stats_window_size, tensorboard_log, verbose, device, support_multi_env, monitor_wrapper, seed, use_sde, sde_sample_freq, supported_action_spaces)\u001b[0m\n\u001b[1;32m    121\u001b[0m     ) -> None:\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_policy_from_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/base_class.py\u001b[0m in \u001b[0;36m_get_policy_from_name\u001b[0;34m(self, policy_name)\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_aliases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpolicy_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Policy {policy_name} unknown\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_torch_save_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Policy pre-trained-1 unknown"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "smb.play(episodes=1, deterministic=True, render=True, return_eval=True)"
      ],
      "metadata": {
        "id": "iJxh7YtXns-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "#Import everything needed to edit/save/watch video clips\n",
        "from moviepy import editor\n",
        "import moviepy\n",
        "\n",
        "def region_selection(image):\n",
        "\t\"\"\"\n",
        "\tDetermine and cut the region of interest in the input image.\n",
        "\tParameters:\n",
        "\t\timage: we pass here the output from canny where we have\n",
        "\t\tidentified edges in the frame\n",
        "\t\"\"\"\n",
        "\t# create an array of the same size as of the input image\n",
        "\tmask = np.zeros_like(image)\n",
        "\t# if you pass an image with more then one channel\n",
        "\tif len(image.shape) > 2:\n",
        "\t\tchannel_count = image.shape[2]\n",
        "\t\tignore_mask_color = (255,) * channel_count\n",
        "\t# our image only has one channel so it will go under \"else\"\n",
        "\telse:\n",
        "\t\t# color of the mask polygon (white)\n",
        "\t\tignore_mask_color = 255\n",
        "\t# creating a polygon to focus only on the road in the picture\n",
        "\t# we have created this polygon in accordance to how the camera was placed\n",
        "\trows, cols = image.shape[:2]\n",
        "\tbottom_left = [cols * 0.1, rows * 0.95]\n",
        "\ttop_left\t = [cols * 0.4, rows * 0.6]\n",
        "\tbottom_right = [cols * 0.9, rows * 0.95]\n",
        "\ttop_right = [cols * 0.6, rows * 0.6]\n",
        "\tvertices = np.array([[bottom_left, top_left, top_right, bottom_right]], dtype=np.int32)\n",
        "\t# filling the polygon with white color and generating the final mask\n",
        "\tcv2.fillPoly(mask, vertices, ignore_mask_color)\n",
        "\t# performing Bitwise AND on the input image and mask to get only the edges on the road\n",
        "\tmasked_image = cv2.bitwise_and(image, mask)\n",
        "\treturn masked_image\n",
        "\n",
        "def hough_transform(image):\n",
        "\t\"\"\"\n",
        "\tDetermine and cut the region of interest in the input image.\n",
        "\tParameter:\n",
        "\t\timage: grayscale image which should be an output from the edge detector\n",
        "\t\"\"\"\n",
        "\t# Distance resolution of the accumulator in pixels.\n",
        "\trho = 1\n",
        "\t# Angle resolution of the accumulator in radians.\n",
        "\ttheta = np.pi/180\n",
        "\t# Only lines that are greater than threshold will be returned.\n",
        "\tthreshold = 20\n",
        "\t# Line segments shorter than that are rejected.\n",
        "\tminLineLength = 20\n",
        "\t# Maximum allowed gap between points on the same line to link them\n",
        "\tmaxLineGap = 500\n",
        "\t# function returns an array containing dimensions of straight lines\n",
        "\t# appearing in the input image\n",
        "\treturn cv2.HoughLinesP(image, rho = rho, theta = theta, threshold = threshold,\n",
        "\t\t\t\t\t\tminLineLength = minLineLength, maxLineGap = maxLineGap)\n",
        "\n",
        "def average_slope_intercept(lines):\n",
        "\t\"\"\"\n",
        "\tFind the slope and intercept of the left and right lanes of each image.\n",
        "\tParameters:\n",
        "\t\tlines: output from Hough Transform\n",
        "\t\"\"\"\n",
        "\tleft_lines = [] #(slope, intercept)\n",
        "\tleft_weights = [] #(length,)\n",
        "\tright_lines = [] #(slope, intercept)\n",
        "\tright_weights = [] #(length,)\n",
        "\n",
        "\tfor line in lines:\n",
        "\t\tfor x1, y1, x2, y2 in line:\n",
        "\t\t\tif x1 == x2:\n",
        "\t\t\t\tcontinue\n",
        "\t\t\t# calculating slope of a line\n",
        "\t\t\tslope = (y2 - y1) / (x2 - x1)\n",
        "\t\t\t# calculating intercept of a line\n",
        "\t\t\tintercept = y1 - (slope * x1)\n",
        "\t\t\t# calculating length of a line\n",
        "\t\t\tlength = np.sqrt(((y2 - y1) ** 2) + ((x2 - x1) ** 2))\n",
        "\t\t\t# slope of left lane is negative and for right lane slope is positive\n",
        "\t\t\tif slope < 0:\n",
        "\t\t\t\tleft_lines.append((slope, intercept))\n",
        "\t\t\t\tleft_weights.append((length))\n",
        "\t\t\telse:\n",
        "\t\t\t\tright_lines.append((slope, intercept))\n",
        "\t\t\t\tright_weights.append((length))\n",
        "\t#\n",
        "\tleft_lane = np.dot(left_weights, left_lines) / np.sum(left_weights) if len(left_weights) > 0 else None\n",
        "\tright_lane = np.dot(right_weights, right_lines) / np.sum(right_weights) if len(right_weights) > 0 else None\n",
        "\treturn left_lane, right_lane\n",
        "\n",
        "def pixel_points(y1, y2, line):\n",
        "\t\"\"\"\n",
        "\tConverts the slope and intercept of each line into pixel points.\n",
        "\t\tParameters:\n",
        "\t\t\ty1: y-value of the line's starting point.\n",
        "\t\t\ty2: y-value of the line's end point.\n",
        "\t\t\tline: The slope and intercept of the line.\n",
        "\t\"\"\"\n",
        "\tif line is None:\n",
        "\t\treturn None\n",
        "\tslope, intercept = line\n",
        "\tx1 = int((y1 - intercept)/slope)\n",
        "\tx2 = int((y2 - intercept)/slope)\n",
        "\ty1 = int(y1)\n",
        "\ty2 = int(y2)\n",
        "\treturn ((x1, y1), (x2, y2))\n",
        "\n",
        "def lane_lines(image, lines):\n",
        "\t\"\"\"\n",
        "\tCreate full lenght lines from pixel points.\n",
        "\t\tParameters:\n",
        "\t\t\timage: The input test image.\n",
        "\t\t\tlines: The output lines from Hough Transform.\n",
        "\t\"\"\"\n",
        "\tleft_lane, right_lane = average_slope_intercept(lines)\n",
        "\ty1 = image.shape[0]\n",
        "\ty2 = y1 * 0.6\n",
        "\tleft_line = pixel_points(y1, y2, left_lane)\n",
        "\tright_line = pixel_points(y1, y2, right_lane)\n",
        "\treturn left_line, right_line\n",
        "\n",
        "\n",
        "def draw_lane_lines(image, lines, color=[255, 0, 0], thickness=12):\n",
        "\t\"\"\"\n",
        "\tDraw lines onto the input image.\n",
        "\t\tParameters:\n",
        "\t\t\timage: The input test image (video frame in our case).\n",
        "\t\t\tlines: The output lines from Hough Transform.\n",
        "\t\t\tcolor (Default = red): Line color.\n",
        "\t\t\tthickness (Default = 12): Line thickness.\n",
        "\t\"\"\"\n",
        "\tline_image = np.zeros_like(image)\n",
        "\tfor line in lines:\n",
        "\t\tif line is not None:\n",
        "\t\t\tcv2.line(line_image, *line, color, thickness)\n",
        "\treturn cv2.addWeighted(image, 1.0, line_image, 1.0, 0.0)\n",
        "\n",
        "def frame_processor(image):\n",
        "\t\"\"\"\n",
        "\tProcess the input frame to detect lane lines.\n",
        "\tParameters:\n",
        "\t\timage: image of a road where one wants to detect lane lines\n",
        "\t\t(we will be passing frames of video to this function)\n",
        "\t\"\"\"\n",
        "\t# convert the RGB image to Gray scale\n",
        "\tgrayscale = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\t# applying gaussian Blur which removes noise from the image\n",
        "\t# and focuses on our region of interest\n",
        "\t# size of gaussian kernel\n",
        "\tkernel_size = 5\n",
        "\t# Applying gaussian blur to remove noise from the frames\n",
        "\tblur = cv2.GaussianBlur(grayscale, (kernel_size, kernel_size), 0)\n",
        "\t# first threshold for the hysteresis procedure\n",
        "\tlow_t = 50\n",
        "\t# second threshold for the hysteresis procedure\n",
        "\thigh_t = 150\n",
        "\t# applying canny edge detection and save edges in a variable\n",
        "\tedges = cv2.Canny(blur, low_t, high_t)\n",
        "\t# since we are getting too many edges from our image, we apply\n",
        "\t# a mask polygon to only focus on the road\n",
        "\t# Will explain Region selection in detail in further steps\n",
        "\tregion = region_selection(edges)\n",
        "\t# Applying hough transform to get straight lines from our image\n",
        "\t# and find the lane lines\n",
        "\t# Will explain Hough Transform in detail in further steps\n",
        "\though = hough_transform(region)\n",
        "\t#lastly we draw the lines on our resulting frame and return it as output\n",
        "\tresult = draw_lane_lines(image, lane_lines(image, hough))\n",
        "\treturn result\n",
        "\n",
        "# driver function\n",
        "def process_video(test_video, output_video):\n",
        "\t\"\"\"\n",
        "\tRead input video stream and produce a video file with detected lane lines.\n",
        "\tParameters:\n",
        "\t\ttest_video: location of input video file\n",
        "\t\toutput_video: location where output video file is to be saved\n",
        "\t\"\"\"\n",
        "\t# read the video file using VideoFileClip without audio\n",
        "\tinput_video = editor.VideoFileClip(\"/content/test2.mp4\", audio=False)\n",
        "\t# apply the function \"frame_processor\" to each frame of the video\n",
        "\t# will give more detail about \"frame_processor\" in further steps\n",
        "\t# \"processed\" stores the output video\n",
        "\tprocessed = input_video.fl_image(frame_processor)\n",
        "\t# save the output video stream to an mp4 file\n",
        "\tprocessed.write_videofile(output_video, audio=False)\n",
        "\n",
        "# calling driver function\n",
        "process_video('input.mp4','output.mp4')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Zc2PEdvk_vl",
        "outputId": "d3ea3567-87eb-45ae-e81b-99de2222d8d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Building video output.mp4.\n",
            "Moviepy - Writing video output.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "t: 100%|█████████▉| 1296/1297 [01:29<00:00,  9.66it/s, now=None]WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file /content/test2.mp4, 2764800 bytes wanted but 0 bytes read,at frame 1295/1298, at time 25.90/25.94 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file /content/test2.mp4, 2764800 bytes wanted but 0 bytes read,at frame 1296/1298, at time 25.92/25.94 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready output.mp4\n"
          ]
        }
      ]
    }
  ]
}