{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from collections import deque\n",
        "\n",
        "def sample(logits):\n",
        "    noise = tf.random_uniform(tf.shape(logits))\n",
        "    return tf.argmax(logits - tf.log(-tf.log(noise)), 1)\n",
        "\n",
        "def cat_entropy(logits):\n",
        "    a0 = logits - tf.reduce_max(logits, 1, keepdims=True)\n",
        "    ea0 = tf.exp(a0)\n",
        "    z0 = tf.reduce_sum(ea0, 1, keepdims=True)\n",
        "    p0 = ea0 / z0\n",
        "    return tf.reduce_sum(p0 * (tf.log(z0) - a0), 1)\n",
        "\n",
        "def cat_entropy_softmax(p0):\n",
        "    return - tf.reduce_sum(p0 * tf.log(p0 + 1e-6), axis = 1)\n",
        "\n",
        "def ortho_init(scale=1.0):\n",
        "    def _ortho_init(shape, dtype, partition_info=None):\n",
        "        #lasagne ortho init for tf\n",
        "        shape = tuple(shape)\n",
        "        if len(shape) == 2:\n",
        "            flat_shape = shape\n",
        "        elif len(shape) == 4: # assumes NHWC\n",
        "            flat_shape = (np.prod(shape[:-1]), shape[-1])\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "        a = np.random.normal(0.0, 1.0, flat_shape)\n",
        "        u, _, v = np.linalg.svd(a, full_matrices=False)\n",
        "        q = u if u.shape == flat_shape else v # pick the one with the correct shape\n",
        "        q = q.reshape(shape)\n",
        "        return (scale * q[:shape[0], :shape[1]]).astype(np.float32)\n",
        "    return _ortho_init\n",
        "\n",
        "def conv(x, scope, *, nf, rf, stride, pad='VALID', init_scale=1.0, data_format='NHWC', one_dim_bias=False):\n",
        "    if data_format == 'NHWC':\n",
        "        channel_ax = 3\n",
        "        strides = [1, stride, stride, 1]\n",
        "        bshape = [1, 1, 1, nf]\n",
        "    elif data_format == 'NCHW':\n",
        "        channel_ax = 1\n",
        "        strides = [1, 1, stride, stride]\n",
        "        bshape = [1, nf, 1, 1]\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    bias_var_shape = [nf] if one_dim_bias else [1, nf, 1, 1]\n",
        "    nin = x.get_shape()[channel_ax].value\n",
        "    wshape = [rf, rf, nin, nf]\n",
        "    with tf.variable_scope(scope):\n",
        "        w = tf.get_variable(\"w\", wshape, initializer=ortho_init(init_scale))\n",
        "        b = tf.get_variable(\"b\", bias_var_shape, initializer=tf.constant_initializer(0.0))\n",
        "        if not one_dim_bias and data_format == 'NHWC':\n",
        "            b = tf.reshape(b, bshape)\n",
        "        return tf.nn.conv2d(x, w, strides=strides, padding=pad, data_format=data_format) + b\n",
        "\n",
        "def fc(x, scope, nh, *, init_scale=1.0, init_bias=0.0):\n",
        "    with tf.variable_scope(scope):\n",
        "        nin = x.get_shape()[1].value\n",
        "        w = tf.get_variable(\"w\", [nin, nh], initializer=ortho_init(init_scale))\n",
        "        b = tf.get_variable(\"b\", [nh], initializer=tf.constant_initializer(init_bias))\n",
        "        return tf.matmul(x, w)+b\n",
        "\n",
        "def batch_to_seq(h, nbatch, nsteps, flat=False):\n",
        "    if flat:\n",
        "        h = tf.reshape(h, [nbatch, nsteps])\n",
        "    else:\n",
        "        h = tf.reshape(h, [nbatch, nsteps, -1])\n",
        "    return [tf.squeeze(v, [1]) for v in tf.split(axis=1, num_or_size_splits=nsteps, value=h)]\n",
        "\n",
        "def seq_to_batch(h, flat = False):\n",
        "    shape = h[0].get_shape().as_list()\n",
        "    if not flat:\n",
        "        assert(len(shape) > 1)\n",
        "        nh = h[0].get_shape()[-1].value\n",
        "        return tf.reshape(tf.concat(axis=1, values=h), [-1, nh])\n",
        "    else:\n",
        "        return tf.reshape(tf.stack(values=h, axis=1), [-1])\n",
        "\n",
        "def lstm(xs, ms, s, scope, nh, init_scale=1.0):\n",
        "    nbatch, nin = [v.value for v in xs[0].get_shape()]\n",
        "    with tf.variable_scope(scope):\n",
        "        wx = tf.get_variable(\"wx\", [nin, nh*4], initializer=ortho_init(init_scale))\n",
        "        wh = tf.get_variable(\"wh\", [nh, nh*4], initializer=ortho_init(init_scale))\n",
        "        b = tf.get_variable(\"b\", [nh*4], initializer=tf.constant_initializer(0.0))\n",
        "\n",
        "    c, h = tf.split(axis=1, num_or_size_splits=2, value=s)\n",
        "    for idx, (x, m) in enumerate(zip(xs, ms)):\n",
        "        c = c*(1-m)\n",
        "        h = h*(1-m)\n",
        "        z = tf.matmul(x, wx) + tf.matmul(h, wh) + b\n",
        "        i, f, o, u = tf.split(axis=1, num_or_size_splits=4, value=z)\n",
        "        i = tf.nn.sigmoid(i)\n",
        "        f = tf.nn.sigmoid(f)\n",
        "        o = tf.nn.sigmoid(o)\n",
        "        u = tf.tanh(u)\n",
        "        c = f*c + i*u\n",
        "        h = o*tf.tanh(c)\n",
        "        xs[idx] = h\n",
        "    s = tf.concat(axis=1, values=[c, h])\n",
        "    return xs, s\n",
        "\n",
        "def _ln(x, g, b, e=1e-5, axes=[1]):\n",
        "    u, s = tf.nn.moments(x, axes=axes, keep_dims=True)\n",
        "    x = (x-u)/tf.sqrt(s+e)\n",
        "    x = x*g+b\n",
        "    return x\n",
        "\n",
        "def lnlstm(xs, ms, s, scope, nh, init_scale=1.0):\n",
        "    nbatch, nin = [v.value for v in xs[0].get_shape()]\n",
        "    with tf.variable_scope(scope):\n",
        "        wx = tf.get_variable(\"wx\", [nin, nh*4], initializer=ortho_init(init_scale))\n",
        "        gx = tf.get_variable(\"gx\", [nh*4], initializer=tf.constant_initializer(1.0))\n",
        "        bx = tf.get_variable(\"bx\", [nh*4], initializer=tf.constant_initializer(0.0))\n",
        "\n",
        "        wh = tf.get_variable(\"wh\", [nh, nh*4], initializer=ortho_init(init_scale))\n",
        "        gh = tf.get_variable(\"gh\", [nh*4], initializer=tf.constant_initializer(1.0))\n",
        "        bh = tf.get_variable(\"bh\", [nh*4], initializer=tf.constant_initializer(0.0))\n",
        "\n",
        "        b = tf.get_variable(\"b\", [nh*4], initializer=tf.constant_initializer(0.0))\n",
        "\n",
        "        gc = tf.get_variable(\"gc\", [nh], initializer=tf.constant_initializer(1.0))\n",
        "        bc = tf.get_variable(\"bc\", [nh], initializer=tf.constant_initializer(0.0))\n",
        "\n",
        "    c, h = tf.split(axis=1, num_or_size_splits=2, value=s)\n",
        "    for idx, (x, m) in enumerate(zip(xs, ms)):\n",
        "        c = c*(1-m)\n",
        "        h = h*(1-m)\n",
        "        z = _ln(tf.matmul(x, wx), gx, bx) + _ln(tf.matmul(h, wh), gh, bh) + b\n",
        "        i, f, o, u = tf.split(axis=1, num_or_size_splits=4, value=z)\n",
        "        i = tf.nn.sigmoid(i)\n",
        "        f = tf.nn.sigmoid(f)\n",
        "        o = tf.nn.sigmoid(o)\n",
        "        u = tf.tanh(u)\n",
        "        c = f*c + i*u\n",
        "        h = o*tf.tanh(_ln(c, gc, bc))\n",
        "        xs[idx] = h\n",
        "    s = tf.concat(axis=1, values=[c, h])\n",
        "    return xs, s\n",
        "\n",
        "def conv_to_fc(x):\n",
        "    nh = np.prod([v.value for v in x.get_shape()[1:]])\n",
        "    x = tf.reshape(x, [-1, nh])\n",
        "    return x\n",
        "\n",
        "def discount_with_dones(rewards, dones, gamma):\n",
        "    discounted = []\n",
        "    r = 0\n",
        "    for reward, done in zip(rewards[::-1], dones[::-1]):\n",
        "        r = reward + gamma*r*(1.-done) # fixed off by one bug\n",
        "        discounted.append(r)\n",
        "    return discounted[::-1]\n",
        "\n",
        "def find_trainable_variables(key):\n",
        "    return tf.trainable_variables(key)\n",
        "\n",
        "def make_path(f):\n",
        "    return os.makedirs(f, exist_ok=True)\n",
        "\n",
        "def constant(p):\n",
        "    return 1\n",
        "\n",
        "def linear(p):\n",
        "    return 1-p\n",
        "\n",
        "def middle_drop(p):\n",
        "    eps = 0.75\n",
        "    if 1-p<eps:\n",
        "        return eps*0.1\n",
        "    return 1-p\n",
        "\n",
        "def double_linear_con(p):\n",
        "    p *= 2\n",
        "    eps = 0.125\n",
        "    if 1-p<eps:\n",
        "        return eps\n",
        "    return 1-p\n",
        "\n",
        "def double_middle_drop(p):\n",
        "    eps1 = 0.75\n",
        "    eps2 = 0.25\n",
        "    if 1-p<eps1:\n",
        "        if 1-p<eps2:\n",
        "            return eps2*0.5\n",
        "        return eps1*0.1\n",
        "    return 1-p\n",
        "\n",
        "schedules = {\n",
        "    'linear':linear,\n",
        "    'constant':constant,\n",
        "    'double_linear_con': double_linear_con,\n",
        "    'middle_drop': middle_drop,\n",
        "    'double_middle_drop': double_middle_drop\n",
        "}\n",
        "\n",
        "class Scheduler(object):\n",
        "\n",
        "    def __init__(self, v, nvalues, schedule):\n",
        "        self.n = 0.\n",
        "        self.v = v\n",
        "        self.nvalues = nvalues\n",
        "        self.schedule = schedules[schedule]\n",
        "\n",
        "    def value(self):\n",
        "        current_value = self.v*self.schedule(self.n/self.nvalues)\n",
        "        self.n += 1.\n",
        "        return current_value\n",
        "\n",
        "    def value_steps(self, steps):\n",
        "        return self.v*self.schedule(steps/self.nvalues)\n",
        "\n",
        "\n",
        "class EpisodeStats:\n",
        "    def __init__(self, nsteps, nenvs):\n",
        "        self.episode_rewards = []\n",
        "        for i in range(nenvs):\n",
        "            self.episode_rewards.append([])\n",
        "        self.lenbuffer = deque(maxlen=40)  # rolling buffer for episode lengths\n",
        "        self.rewbuffer = deque(maxlen=40)  # rolling buffer for episode rewards\n",
        "        self.nsteps = nsteps\n",
        "        self.nenvs = nenvs\n",
        "\n",
        "    def feed(self, rewards, masks):\n",
        "        rewards = np.reshape(rewards, [self.nenvs, self.nsteps])\n",
        "        masks = np.reshape(masks, [self.nenvs, self.nsteps])\n",
        "        for i in range(0, self.nenvs):\n",
        "            for j in range(0, self.nsteps):\n",
        "                self.episode_rewards[i].append(rewards[i][j])\n",
        "                if masks[i][j]:\n",
        "                    l = len(self.episode_rewards[i])\n",
        "                    s = sum(self.episode_rewards[i])\n",
        "                    self.lenbuffer.append(l)\n",
        "                    self.rewbuffer.append(s)\n",
        "                    self.episode_rewards[i] = []\n",
        "\n",
        "    def mean_length(self):\n",
        "        if self.lenbuffer:\n",
        "            return np.mean(self.lenbuffer)\n",
        "        else:\n",
        "            return 0  # on the first params dump, no episodes are finished\n",
        "\n",
        "    def mean_reward(self):\n",
        "        if self.rewbuffer:\n",
        "            return np.mean(self.rewbuffer)\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "\n",
        "# For ACER\n",
        "def get_by_index(x, idx):\n",
        "    assert(len(x.get_shape()) == 2)\n",
        "    assert(len(idx.get_shape()) == 1)\n",
        "    idx_flattened = tf.range(0, x.shape[0]) * x.shape[1] + idx\n",
        "    y = tf.gather(tf.reshape(x, [-1]),  # flatten input\n",
        "                  idx_flattened)  # use flattened indices\n",
        "    return y\n",
        "\n",
        "def check_shape(ts,shapes):\n",
        "    i = 0\n",
        "    for (t,shape) in zip(ts,shapes):\n",
        "        assert t.get_shape().as_list()==shape, \"id \" + str(i) + \" shape \" + str(t.get_shape()) + str(shape)\n",
        "        i += 1\n",
        "\n",
        "def avg_norm(t):\n",
        "    return tf.reduce_mean(tf.sqrt(tf.reduce_sum(tf.square(t), axis=-1)))\n",
        "\n",
        "def gradient_add(g1, g2, param):\n",
        "    print([g1, g2, param.name])\n",
        "    assert (not (g1 is None and g2 is None)), param.name\n",
        "    if g1 is None:\n",
        "        return g2\n",
        "    elif g2 is None:\n",
        "        return g1\n",
        "    else:\n",
        "        return g1 + g2\n",
        "\n",
        "def q_explained_variance(qpred, q):\n",
        "    _, vary = tf.nn.moments(q, axes=[0, 1])\n",
        "    _, varpred = tf.nn.moments(q - qpred, axes=[0, 1])\n",
        "    check_shape([vary, varpred], [[]] * 2)\n",
        "    return 1.0 - (varpred / vary)"
      ],
      "metadata": {
        "id": "9h_iF5FC0uLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/openai/baselines.git@master"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "03XdqDIZqblH",
        "outputId": "357bfd3a-09d6-4f5e-a03a-01c40f95dc39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/baselines.git@master\n",
            "  Cloning https://github.com/openai/baselines.git (to revision master) to /tmp/pip-req-build-z44qs0vv\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/baselines.git /tmp/pip-req-build-z44qs0vv\n",
            "  Resolved https://github.com/openai/baselines.git to commit ea25b9e8b234e6ee1bca43083f8f3cf974143998\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gym<0.16.0,>=0.15.4 (from baselines==0.1.6)\n",
            "  Downloading gym-0.15.7.tar.gz (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from baselines==0.1.6) (1.11.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from baselines==0.1.6) (4.66.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from baselines==0.1.6) (1.3.2)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from baselines==0.1.6) (2.2.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from baselines==0.1.6) (8.1.7)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from baselines==0.1.6) (4.8.0.76)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.10/dist-packages (from gym<0.16.0,>=0.15.4->baselines==0.1.6) (1.25.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gym<0.16.0,>=0.15.4->baselines==0.1.6) (1.16.0)\n",
            "Collecting pyglet<=1.5.0,>=1.4.0 (from gym<0.16.0,>=0.15.4->baselines==0.1.6)\n",
            "  Downloading pyglet-1.5.0-py2.py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cloudpickle (from baselines==0.1.6)\n",
            "  Downloading cloudpickle-1.2.2-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym<0.16.0,>=0.15.4->baselines==0.1.6) (0.18.3)\n",
            "Building wheels for collected packages: baselines, gym\n",
            "  Building wheel for baselines (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for baselines: filename=baselines-0.1.6-py3-none-any.whl size=220652 sha256=d4c7f91c608577c3675ef44359b14be18923c70b23e6804a3d0450b51e03842f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-kudlk63l/wheels/30/86/63/c0f1a6b8a06d3e3deaf463ad1f034f6cbd720a7e5fcf1c70d3\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.15.7-py3-none-any.whl size=1648805 sha256=7a26d30ee94d4257cdf1b48cdda94b9276c43ae0c037d400fa9e2034c09b94b9\n",
            "  Stored in directory: /root/.cache/pip/wheels/09/34/69/e1c9061afff9529cff995b1861b5e88c31b845dbb28ea1c9d6\n",
            "Successfully built baselines gym\n",
            "Installing collected packages: cloudpickle, pyglet, gym, baselines\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 2.2.1\n",
            "    Uninstalling cloudpickle-2.2.1:\n",
            "      Successfully uninstalled cloudpickle-2.2.1\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "  Attempting uninstall: baselines\n",
            "    Found existing installation: baselines 0.1.5\n",
            "    Uninstalling baselines-0.1.5:\n",
            "      Successfully uninstalled baselines-0.1.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "bigframes 0.24.0 requires cloudpickle>=2.0.0, but you have cloudpickle 1.2.2 which is incompatible.\n",
            "dask 2023.8.1 requires cloudpickle>=1.5.0, but you have cloudpickle 1.2.2 which is incompatible.\n",
            "distributed 2023.8.1 requires cloudpickle>=1.5.0, but you have cloudpickle 1.2.2 which is incompatible.\n",
            "tensorflow-probability 0.23.0 requires cloudpickle>=1.3, but you have cloudpickle 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed baselines-0.1.6 cloudpickle-1.2.2 gym-0.15.7 pyglet-1.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "baselines",
                  "cloudpickle",
                  "gym"
                ]
              },
              "id": "a64fa0b9a2fd47e8a4a229b05af7de94"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import baselines.common.runners"
      ],
      "metadata": {
        "id": "I5C6pQkEqUY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from baselines.a2c.utils import discount_with_dones\n",
        "from baselines.common.runners import AbstractEnvRunner\n",
        "\n",
        "class Runner(AbstractEnvRunner):\n",
        "    \"\"\"\n",
        "    We use this class to generate batches of experiences\n",
        "\n",
        "    __init__:\n",
        "    - Initialize the runner\n",
        "\n",
        "    run():\n",
        "    - Make a mini batch of experiences\n",
        "    \"\"\"\n",
        "    def __init__(self, env, model, nsteps=5, gamma=0.99):\n",
        "        super().__init__(env=env, model=model, nsteps=nsteps)\n",
        "        self.gamma = gamma\n",
        "        self.batch_action_shape = [x if x is not None else -1 for x in model.train_model.action.shape.as_list()]\n",
        "        self.ob_dtype = model.train_model.X.dtype.as_numpy_dtype\n",
        "\n",
        "    def run(self):\n",
        "        # We initialize the lists that will contain the mb of experiences\n",
        "        mb_obs, mb_rewards, mb_actions, mb_values, mb_dones = [],[],[],[],[]\n",
        "        mb_states = self.states\n",
        "        epinfos = []\n",
        "        for n in range(self.nsteps):\n",
        "            # Given observations, take action and value (V(s))\n",
        "            # We already have self.obs because Runner superclass run self.obs[:] = env.reset() on init\n",
        "            actions, values, states, _ = self.model.step(self.obs, S=self.states, M=self.dones)\n",
        "\n",
        "            # Append the experiences\n",
        "            mb_obs.append(np.copy(self.obs))\n",
        "            mb_actions.append(actions)\n",
        "            mb_values.append(values)\n",
        "            mb_dones.append(self.dones)\n",
        "\n",
        "            # Take actions in env and look the results\n",
        "            obs, rewards, dones, infos = self.env.step(actions)\n",
        "            for info in infos:\n",
        "                maybeepinfo = info.get('episode')\n",
        "                if maybeepinfo: epinfos.append(maybeepinfo)\n",
        "            self.states = states\n",
        "            self.dones = dones\n",
        "            self.obs = obs\n",
        "            mb_rewards.append(rewards)\n",
        "        mb_dones.append(self.dones)\n",
        "\n",
        "        # Batch of steps to batch of rollouts\n",
        "        mb_obs = np.asarray(mb_obs, dtype=self.ob_dtype).swapaxes(1, 0).reshape(self.batch_ob_shape)\n",
        "        mb_rewards = np.asarray(mb_rewards, dtype=np.float32).swapaxes(1, 0)\n",
        "        mb_actions = np.asarray(mb_actions, dtype=self.model.train_model.action.dtype.name).swapaxes(1, 0)\n",
        "        mb_values = np.asarray(mb_values, dtype=np.float32).swapaxes(1, 0)\n",
        "        mb_dones = np.asarray(mb_dones, dtype=np.bool).swapaxes(1, 0)\n",
        "        mb_masks = mb_dones[:, :-1]\n",
        "        mb_dones = mb_dones[:, 1:]\n",
        "\n",
        "\n",
        "        if self.gamma > 0.0:\n",
        "            # Discount/bootstrap off value fn\n",
        "            last_values = self.model.value(self.obs, S=self.states, M=self.dones).tolist()\n",
        "            for n, (rewards, dones, value) in enumerate(zip(mb_rewards, mb_dones, last_values)):\n",
        "                rewards = rewards.tolist()\n",
        "                dones = dones.tolist()\n",
        "                if dones[-1] == 0:\n",
        "                    rewards = discount_with_dones(rewards+[value], dones+[0], self.gamma)[:-1]\n",
        "                else:\n",
        "                    rewards = discount_with_dones(rewards, dones, self.gamma)\n",
        "\n",
        "                mb_rewards[n] = rewards\n",
        "\n",
        "        mb_actions = mb_actions.reshape(self.batch_action_shape)\n",
        "\n",
        "        mb_rewards = mb_rewards.flatten()\n",
        "        mb_values = mb_values.flatten()\n",
        "        mb_masks = mb_masks.flatten()\n",
        "        return mb_obs, mb_states, mb_rewards, mb_masks, mb_actions, mb_values, epinfos"
      ],
      "metadata": {
        "id": "EUJJFnnN2hMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import functools\n",
        "import tensorflow as tf\n",
        "\n",
        "from baselines import logger\n",
        "\n",
        "from baselines.common import set_global_seeds, explained_variance\n",
        "from baselines.common import tf_util\n",
        "from baselines.common.policies import build_policy\n",
        "\n",
        "\n",
        "from baselines.a2c.utils import Scheduler, find_trainable_variables\n",
        "from baselines.a2c.runner import Runner\n",
        "from baselines.ppo2.ppo2 import safemean\n",
        "from collections import deque\n",
        "\n",
        "from tensorflow import losses\n",
        "\n",
        "class Model(object):\n",
        "\n",
        "    \"\"\"\n",
        "    We use this class to :\n",
        "        __init__:\n",
        "        - Creates the step_model\n",
        "        - Creates the train_model\n",
        "\n",
        "        train():\n",
        "        - Make the training part (feedforward and retropropagation of gradients)\n",
        "\n",
        "        save/load():\n",
        "        - Save load the model\n",
        "    \"\"\"\n",
        "    def __init__(self, policy, env, nsteps,\n",
        "            ent_coef=0.01, vf_coef=0.5, max_grad_norm=0.5, lr=7e-4,\n",
        "            alpha=0.99, epsilon=1e-5, total_timesteps=int(80e6), lrschedule='linear'):\n",
        "\n",
        "        sess = tf_util.get_session()\n",
        "        nenvs = env.num_envs\n",
        "        nbatch = nenvs*nsteps\n",
        "\n",
        "\n",
        "        with tf.variable_scope('a2c_model', reuse=tf.AUTO_REUSE):\n",
        "            # step_model is used for sampling\n",
        "            step_model = policy(nenvs, 1, sess)\n",
        "\n",
        "            # train_model is used to train our network\n",
        "            train_model = policy(nbatch, nsteps, sess)\n",
        "\n",
        "        A = tf.placeholder(train_model.action.dtype, train_model.action.shape)\n",
        "        ADV = tf.placeholder(tf.float32, [nbatch])\n",
        "        R = tf.placeholder(tf.float32, [nbatch])\n",
        "        LR = tf.placeholder(tf.float32, [])\n",
        "\n",
        "        # Calculate the loss\n",
        "        # Total loss = Policy gradient loss - entropy * entropy coefficient + Value coefficient * value loss\n",
        "\n",
        "        # Policy loss\n",
        "        neglogpac = train_model.pd.neglogp(A)\n",
        "        # L = A(s,a) * -logpi(a|s)\n",
        "        pg_loss = tf.reduce_mean(ADV * neglogpac)\n",
        "\n",
        "        # Entropy is used to improve exploration by limiting the premature convergence to suboptimal policy.\n",
        "        entropy = tf.reduce_mean(train_model.pd.entropy())\n",
        "\n",
        "        # Value loss\n",
        "        vf_loss = losses.mean_squared_error(tf.squeeze(train_model.vf), R)\n",
        "\n",
        "        loss = pg_loss - entropy*ent_coef + vf_loss * vf_coef\n",
        "\n",
        "        # Update parameters using loss\n",
        "        # 1. Get the model parameters\n",
        "        params = find_trainable_variables(\"a2c_model\")\n",
        "\n",
        "        # 2. Calculate the gradients\n",
        "        grads = tf.gradients(loss, params)\n",
        "        if max_grad_norm is not None:\n",
        "            # Clip the gradients (normalize)\n",
        "            grads, grad_norm = tf.clip_by_global_norm(grads, max_grad_norm)\n",
        "        grads = list(zip(grads, params))\n",
        "        # zip aggregate each gradient with parameters associated\n",
        "        # For instance zip(ABCD, xyza) => Ax, By, Cz, Da\n",
        "\n",
        "        # 3. Make op for one policy and value update step of A2C\n",
        "        trainer = tf.train.RMSPropOptimizer(learning_rate=LR, decay=alpha, epsilon=epsilon)\n",
        "\n",
        "        _train = trainer.apply_gradients(grads)\n",
        "\n",
        "        lr = Scheduler(v=lr, nvalues=total_timesteps, schedule=lrschedule)\n",
        "\n",
        "        def train(obs, states, rewards, masks, actions, values):\n",
        "            # Here we calculate advantage A(s,a) = R + yV(s') - V(s)\n",
        "            # rewards = R + yV(s')\n",
        "            advs = rewards - values\n",
        "            for step in range(len(obs)):\n",
        "                cur_lr = lr.value()\n",
        "\n",
        "            td_map = {train_model.X:obs, A:actions, ADV:advs, R:rewards, LR:cur_lr}\n",
        "            if states is not None:\n",
        "                td_map[train_model.S] = states\n",
        "                td_map[train_model.M] = masks\n",
        "            policy_loss, value_loss, policy_entropy, _ = sess.run(\n",
        "                [pg_loss, vf_loss, entropy, _train],\n",
        "                td_map\n",
        "            )\n",
        "            return policy_loss, value_loss, policy_entropy\n",
        "\n",
        "\n",
        "        self.train = train\n",
        "        self.train_model = train_model\n",
        "        self.step_model = step_model\n",
        "        self.step = step_model.step\n",
        "        self.value = step_model.value\n",
        "        self.initial_state = step_model.initial_state\n",
        "        self.save = functools.partial(tf_util.save_variables, sess=sess)\n",
        "        self.load = functools.partial(tf_util.load_variables, sess=sess)\n",
        "        tf.global_variables_initializer().run(session=sess)\n",
        "\n",
        "\n",
        "def learn(\n",
        "    network,\n",
        "    env,\n",
        "    seed=None,\n",
        "    nsteps=5,\n",
        "    total_timesteps=int(80e6),\n",
        "    vf_coef=0.5,\n",
        "    ent_coef=0.01,\n",
        "    max_grad_norm=0.5,\n",
        "    lr=7e-4,\n",
        "    lrschedule='linear',\n",
        "    epsilon=1e-5,\n",
        "    alpha=0.99,\n",
        "    gamma=0.99,\n",
        "    log_interval=100,\n",
        "    load_path=None,\n",
        "    **network_kwargs):\n",
        "\n",
        "    '''\n",
        "    Main entrypoint for A2C algorithm. Train a policy with given network architecture on a given environment using a2c algorithm.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "\n",
        "    network:            policy network architecture. Either string (mlp, lstm, lnlstm, cnn_lstm, cnn, cnn_small, conv_only - see baselines.common/models.py for full list)\n",
        "                        specifying the standard network architecture, or a function that takes tensorflow tensor as input and returns\n",
        "                        tuple (output_tensor, extra_feed) where output tensor is the last network layer output, extra_feed is None for feed-forward\n",
        "                        neural nets, and extra_feed is a dictionary describing how to feed state into the network for recurrent neural nets.\n",
        "                        See baselines.common/policies.py/lstm for more details on using recurrent nets in policies\n",
        "\n",
        "\n",
        "    env:                RL environment. Should implement interface similar to VecEnv (baselines.common/vec_env) or be wrapped with DummyVecEnv (baselines.common/vec_env/dummy_vec_env.py)\n",
        "\n",
        "\n",
        "    seed:               seed to make random number sequence in the alorightm reproducible. By default is None which means seed from system noise generator (not reproducible)\n",
        "\n",
        "    nsteps:             int, number of steps of the vectorized environment per update (i.e. batch size is nsteps * nenv where\n",
        "                        nenv is number of environment copies simulated in parallel)\n",
        "\n",
        "    total_timesteps:    int, total number of timesteps to train on (default: 80M)\n",
        "\n",
        "    vf_coef:            float, coefficient in front of value function loss in the total loss function (default: 0.5)\n",
        "\n",
        "    ent_coef:           float, coeffictiant in front of the policy entropy in the total loss function (default: 0.01)\n",
        "\n",
        "    max_gradient_norm:  float, gradient is clipped to have global L2 norm no more than this value (default: 0.5)\n",
        "\n",
        "    lr:                 float, learning rate for RMSProp (current implementation has RMSProp hardcoded in) (default: 7e-4)\n",
        "\n",
        "    lrschedule:         schedule of learning rate. Can be 'linear', 'constant', or a function [0..1] -> [0..1] that takes fraction of the training progress as input and\n",
        "                        returns fraction of the learning rate (specified as lr) as output\n",
        "\n",
        "    epsilon:            float, RMSProp epsilon (stabilizes square root computation in denominator of RMSProp update) (default: 1e-5)\n",
        "\n",
        "    alpha:              float, RMSProp decay parameter (default: 0.99)\n",
        "\n",
        "    gamma:              float, reward discounting parameter (default: 0.99)\n",
        "\n",
        "    log_interval:       int, specifies how frequently the logs are printed out (default: 100)\n",
        "\n",
        "    **network_kwargs:   keyword arguments to the policy / network builder. See baselines.common/policies.py/build_policy and arguments to a particular type of network\n",
        "                        For instance, 'mlp' network architecture has arguments num_hidden and num_layers.\n",
        "\n",
        "    '''\n",
        "\n",
        "\n",
        "\n",
        "    set_global_seeds(seed)\n",
        "\n",
        "    # Get the nb of env\n",
        "    nenvs = env.num_envs\n",
        "    policy = build_policy(env, network, **network_kwargs)\n",
        "\n",
        "    # Instantiate the model object (that creates step_model and train_model)\n",
        "    model = Model(policy=policy, env=env, nsteps=nsteps, ent_coef=ent_coef, vf_coef=vf_coef,\n",
        "        max_grad_norm=max_grad_norm, lr=lr, alpha=alpha, epsilon=epsilon, total_timesteps=total_timesteps, lrschedule=lrschedule)\n",
        "    if load_path is not None:\n",
        "        model.load(load_path)\n",
        "\n",
        "    # Instantiate the runner object\n",
        "    runner = Runner(env, model, nsteps=nsteps, gamma=gamma)\n",
        "    epinfobuf = deque(maxlen=100)\n",
        "\n",
        "    # Calculate the batch_size\n",
        "    nbatch = nenvs*nsteps\n",
        "\n",
        "    # Start total timer\n",
        "    tstart = time.time()\n",
        "\n",
        "    for update in range(1, total_timesteps//nbatch+1):\n",
        "        # Get mini batch of experiences\n",
        "        obs, states, rewards, masks, actions, values, epinfos = runner.run()\n",
        "        epinfobuf.extend(epinfos)\n",
        "\n",
        "        policy_loss, value_loss, policy_entropy = model.train(obs, states, rewards, masks, actions, values)\n",
        "        nseconds = time.time()-tstart\n",
        "\n",
        "        # Calculate the fps (frame per second)\n",
        "        fps = int((update*nbatch)/nseconds)\n",
        "        if update % log_interval == 0 or update == 1:\n",
        "            # Calculates if value function is a good predicator of the returns (ev > 1)\n",
        "            # or if it's just worse than predicting nothing (ev =< 0)\n",
        "            ev = explained_variance(values, rewards)\n",
        "            logger.record_tabular(\"nupdates\", update)\n",
        "            logger.record_tabular(\"total_timesteps\", update*nbatch)\n",
        "            logger.record_tabular(\"fps\", fps)\n",
        "            logger.record_tabular(\"policy_entropy\", float(policy_entropy))\n",
        "            logger.record_tabular(\"value_loss\", float(value_loss))\n",
        "            logger.record_tabular(\"explained_variance\", float(ev))\n",
        "            logger.record_tabular(\"eprewmean\", safemean([epinfo['r'] for epinfo in epinfobuf]))\n",
        "            logger.record_tabular(\"eplenmean\", safemean([epinfo['l'] for epinfo in epinfobuf]))\n",
        "            logger.dump_tabular()\n",
        "    return model"
      ],
      "metadata": {
        "id": "uQQxan7-qu_V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}